\section{Experimental Results}

\begin{table*}[t]
    \centering
    \small
    \caption{Architecture Comparison: Functional Correctness (\%) by Domain and Difficulty. Best per category in \textbf{bold}.}
    \label{tab:architecture-comparison}
    \begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}lccccc|cccccc}
        \toprule
        & \multicolumn{6}{c}{\textbf{By Domain}} & & \multicolumn{3}{c}{\textbf{By Difficulty}} \\
        \cmidrule(lr){2-7} \cmidrule(lr){9-11}
        \textbf{Architecture} & \textbf{String} & \textbf{List} & \textbf{Logic} & \textbf{Math} & \textbf{Dsa} & \textbf{Avg} && \textbf{Easy} & \textbf{Med} & \textbf{Hard} \\
        \midrule
        Naive & \textbf{93.2} & 71.4 & 67.6 & 56.9 & 83.5 & \textbf{77.6} && 78.5 & 85.8 & \textbf{69.7} \\
        Single-Agent & 85.2 & \textbf{72.9} & 64.9 & 62.7 & 76.5 & 74.6 && \textbf{89.2} & 83.5 & 59.7 \\
        Multi-Agent & 90.9 & 61.4 & \textbf{75.7} & \textbf{64.7} & \textbf{85.9} & \textbf{77.6} && 76.9 & \textbf{90.6} & 66.2 \\
        \midrule
        $\Delta$ (M-S) & $+5.7$ & $-11.4$ & $+10.8$ & $+2.0$ & $+9.4$ & $+3.0$ && $-12.3$ & $+7.1$ & $+6.5$ \\
        \bottomrule
    \end{tabular*}
    \par\vspace{2mm}
    \parbox{\textwidth}{\footnotesize M=Multi, S=Single. $\Delta$ shows Multi-Agent improvement over Single-Agent (percentage points).}
\end{table*}

\subsection{Dataset}

We constructed a benchmark of 25 programming tasks spanning five domains: string manipulation, list operations, logic problems, mathematical computation, and data
structures \& algorithms (DSA). Tasks were sourced from HumanEval~\citep{chen2021evaluating} and competitive programming platform LeetCode~\citep{leetcode}.
Each domain contains Easy (1 tasks), Medium (2 tasks), and Hard (2 tasks) difficulty levels, stratified by algorithmic complexity and edge case density.
All tasks include function signatures, specifications, input-output examples, and comprehensive test suites, with an average of 13 test cases per task.

% ======================================================

\subsection{Model Selection and Configuration}
To identify the most suitable base model for our pipeline comparisons, we first evaluated three code-specialized LLMs in a single-agent setting: CodeLlama-13B-Instruct~\citep{roziere2023code},
DeepSeek-Coder-v2-16B-Instruct~\citep{deepseekai2024}, and Qwen2.5-Coder-7B-Instruct~\citep{qwen2024} (Table~\ref{tab:model-comparison}).
Qwen2.5-Coder-7B achieved the highest overall pass rate (74.6\%), outperforming DeepSeek-Coder-v2-16B (71.0\%) and CodeLlama-13B (39.6\%). Given its superior performance and parameter efficiency,
we selected Qwen2.5-Coder-7B as the foundation for all subsequent architectural experiments.

All experiments utilize locally-hosted models via Ollama. For the naive and single-agent approaches, we use \texttt{qwen2.5-coder:7b-instruct}. For the multi-agent system, we employ a heterogeneous
configuration: \texttt{qwen2.5-coder:7b-instruct} for the Planner and Coder agents, and \texttt{deepseek-coder-v2:16b} for the Critic. This configuration leverages the larger model's reasoning capabilities
to provide independent, rigorous validation of the generated code.

\begin{table}[htbp]
    \centering
    \small
    \setlength{\tabcolsep}{3pt}
    \caption{Single-Agent Performance Comparison Across LLM Models. Pass rates (\%) by domain. Best in \textbf{bold}.}
    \label{tab:model-comparison}
    \resizebox{\columnwidth}{!}{%
        \begin{tabular}{lccccc|c}
            \toprule
            \textbf{Model}      & \textbf{String} & \textbf{List} & \textbf{Logic} & \textbf{Math} & \textbf{Dsa}  & \textbf{Avg}  \\
            \midrule
            CodeLlama-13B       & 71.6            & 27.1          & 21.6           & 21.6          & 34.3          & 39.6          \\
            DeepSeek-16B        & 86.4            & 65.7          & \textbf{70.3}  & 60.8          & 65.9          & 71.0          \\
            \textbf{Qwen2.5-7B} & \textbf{85.2}   & \textbf{72.9} & 64.9           & \textbf{62.7} & \textbf{76.5} & \textbf{74.6} \\
            \bottomrule
        \end{tabular}%
    }
\end{table}

% ======================================================

\subsection{RQ1: Code Correctness and Quality Comparison}
We evaluate the architectures by analyzing functional correctness across all the tasks and assessing code quality metrics on a subset of complex tasks.

\subsubsection{Functional Correctness}
Table~\ref{tab:architecture-comparison} presents our main results for functional correctness.
Surprisingly, the Naive and Multi-Agent architectures achieve the same overall pass rate of 77.6\%, effectively tying for the best performance, while the Single-Agent approach follows closely at 74.6\%. However, these aggregate metrics
hide substantial performance differences across domains and difficulty levels.

\textbf{Domain-Level Analysis:} Performance varies substantially by task domain. Multi-Agent demonstrates clear advantages in algorithmically complex domains: DSA (85.9\%, $+9.4$pp over Single), Logic (75.7\%, $+10.8$pp over Single), and
Math (64.7\%, $+2.0$pp over Single). However, it underperforms on Lists tasks (61.4\%, $-11.4$pp over Single). The Naive baseline achieves the highest Strings performance (93.2\%), suggesting pattern-matching tasks do not benefit from complex
reasoning architectures.

\textbf{Difficulty-Level Analysis:} The architectures show distinct profiles across difficulty. Single-Agent excels on Easy tasks (89.2\%, $+10.7$pp over Naive), likely due to its systematic analysis phase.
Multi-Agent demonstrates value on Medium (90.6\%, $+7.1$pp over Single) and Hard tasks (66.2\%, $+6.5$pp over Single), where Planner-Coder-Critic coordination enables sophisticated decomposition. \\
Notably, Naive outperforms both on Hard tasks (69.7\%). We attribute this counter-intuitive result to "over-refinement," where the complex validation loops in agentic systems inadvertently introduce regressions on the most difficult problems.

\subsubsection{Code Quality Analysis}
Beyond correctness, we analyzed the code quality of solutions generated by the three architectures for five hard difficulty tasks (longest\_substring\_without\_repeating, triples\_sum\_to\_zero, find\_median\_sorted\_arrays, solve\_n\_queens, largest\_prime\_factor)
using four key metrics: Maintainability Index (MI), Cyclomatic Complexity (CC), Lines of Code (LOC), and Halstead Volume (HV). Table~\ref{tab:code-quality} summarizes the results. \\
The Naive baseline generates the simplest code, with the lowest cyclomatic complexity (5.40) and lines of code (18.40), likely reflecting its tendency to produce direct, but sometimes less robust solutions.
The Single-Agent and Multi-Agent architectures generate more complex code (CC 7.20 and 8.20 respectively) with higher Halstead volumes, reflecting their more comprehensive handling of edge cases and input validation.
Crucially, despite the added complexity, the agentic approaches maintain or improve the Maintainability Index (Single Agent: 61.61, Multi Agent: 61.41) compared to Naive (59.43), suggesting that the additional logic is structured effectively.
The Multi-Agent system's higher complexity correlates with its superior performance on these hard tasks, indicating that the problem difficulty necessitates more sophisticated logic that simple solutions cannot capture.

\begin{table}[htbp]
    \centering
    \small
    \caption{Average Code Quality Metrics on Hard Tasks. Arrows indicate desired direction ($\uparrow$ higher is better, $\downarrow$ lower is better).}
    \label{tab:code-quality}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Architecture} & \textbf{MI} ($\uparrow$) & \textbf{CC} ($\downarrow$) & \textbf{LOC}   & \textbf{HV}     \\
        \midrule
        Naive                 & 59.43                    & \textbf{5.40}              & \textbf{18.40} & \textbf{147.09} \\
        Single-Agent          & \textbf{61.61}           & 7.20                       & 23.20          & 218.84          \\
        Multi-Agent           & 61.41                    & 8.20                       & 26.40          & 273.83          \\
        \bottomrule
    \end{tabular}
\end{table}

% ======================================================

\subsection{RQ2: Impact of Coordination Strategies}
The coordination strategies employed in the Multi-Agent system present a clear trade-off between computational efficiency and algorithmic robustness.
While the increase in LLM calls per task (10-20 vs. 5-8 for the Single-Agent approach) yields diminishing returns on simple tasks, it proves essential in high-complexity domains.
We observe that the overhead of context switching and message passing between agents actively degrades performance on tasks where direct generation suffices. For simple pattern-matching,
the iterative consensus mechanism creates noise rather than signal, leading to the performance regression seen in the Easy category. Conversely, for logic-heavy tasks, this same mechanism
acts as a necessary filter; the Planner-Critic loop effectively catches conceptual errors that a single pass misses, justifying the latency.

Furthermore, the adoption of a heterogeneous configuration significantly enhances the validation process. By separating the critique function (DeepSeek-V2) from the generation engine (Qwen2.5),
the architecture introduces a layer of independence. This structural diversity mitigates the risk of confirmation bias where a homogeneous model validates its own hallucinations. This suggests that effective coordination relies as much on model diversity as
it does on iterative loops.

% ======================================================

\subsection{RQ3: Effect of Role Separation}
Modular role separation improves code generation conditionally rather than universally. The benefits are concentrated in tasks necessitating careful planning (e.g., Logic constraints, DSA optimization) and systematic validation.
For simpler tasks, however, this form of architectural layering introduces overhead that may be counterproductive. Our results indicate that domains such as string manipulation or trivial edge-case handling derive little benefit
from multi-stage reasoning; in these cases, the Planner’s detailed decomposition can over-constrain the solution space, while the Critic’s feedback mechanism may reject valid yet unconventional solutions.

At the same time, however, the quality analysis reveals that role separation successfully regulates code complexity. Although the Multi-Agent system yields solutions with the highest Cyclomatic Complexity and Halstead Volume, it maintains a
Maintainability Index (61.41) comparable to the Single-Agent approach (61.61). This indicates that the specialized agents—structuring, implementing, and reviewing—ensure that the increased complexity required for hard tasks is
compartmentalized, preventing it from degrading the overall maintainability of the codebase.