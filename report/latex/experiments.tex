\section{Experimental Results}

\subsection{Dataset}

We constructed a benchmark of 25 programming tasks spanning five domains: string manipulation, list operations, logic problems, mathematical computation, and data
structures \& algorithms (DSA). Tasks were sourced from HumanEval~\citep{chen2021evaluating} and competitive programming platform LeetCode~\citep{leetcode}.
Each domain contains Easy (1 tasks), Medium (2 tasks), and Hard (2 tasks) difficulty levels, stratified by algorithmic complexity and edge case density.
All tasks include function signatures, specifications, input-output examples, and comprehensive test suites, with an average of 13 test cases per task.

\subsection{Model Selection}
We evaluated three code-specialized LLMs in single-agent mode: CodeLlama-13B~\citep{roziere2023code}, DeepSeek-Coder-v2-16B~\citep{deepseekai2024}, and Qwen2.5-Coder-7B~\citep{qwen2024}
(Table~\ref{tab:model-comparison}). Qwen2.5-Coder-7B achieved the highest overall pass rate (74.6\%), exceeding both DeepSeek-16B (71.0\%) and CodeLlama-13B (39.6\%).
Based on this superior performance and parameter efficiency, we selected Qwen as the base model for architecture comparisons.

\begin{table}[t]
    \centering
    \small
    \setlength{\tabcolsep}{3pt}
    \caption{Single-Agent Performance Comparison Across LLM Models. Pass rates (\%) by domain. Best in \textbf{bold}.}
    \label{tab:model-comparison}
    \resizebox{\columnwidth}{!}{%
        \begin{tabular}{l|ccccc|c}
            \toprule
            \textbf{Model}      & \textbf{String} & \textbf{List} & \textbf{Logic} & \textbf{Math} & \textbf{Dsa}  & \textbf{Avg}  \\
            \midrule
            CodeLlama-13B       & 71.6            & 27.1          & 21.6           & 21.6          & 34.3          & 39.6          \\
            DeepSeek-16B        & 86.4            & 65.7          & \textbf{70.3}  & 60.8          & 65.9          & 71.0          \\
            \textbf{Qwen2.5-7B} & \textbf{85.2}   & \textbf{72.9} & 64.9           & \textbf{62.7} & \textbf{76.5} & \textbf{74.6} \\
            \bottomrule
        \end{tabular}%
    }
\end{table}

\subsection{RQ1: Architecture Quality Comparison}

Table~\ref{tab:architecture-comparison} presents our main results. Surprisingly, overall performance is nearly identical: Multi-Agent and Naive both achieve 77.6\% correctness, while Single-Agent achieves 74.6\% ($-3.0$pp).

\textbf{Domain-Level Analysis.} Performance varies substantially by task domain. Multi-Agent demonstrates clear advantages in algorithmically complex domains: DSA (85.9\%, $+9.4$pp vs Single), Logic (75.7\%, $+10.8$pp), and Math (64.7\%, $+2.0$pp). However, it underperforms on Lists tasks (61.4\%, $-11.4$pp vs Single). The Naive baseline achieves the highest Strings performance (93.2\%), suggesting pattern-matching tasks do not benefit from complex reasoning architectures.

\textbf{Difficulty-Level Analysis.} The architectures show distinct profiles across difficulty. Single-Agent excels on Easy tasks (89.8\%, $+13.0$pp over Naive), likely due to its systematic analysis phase. Multi-Agent demonstrates value on Medium (91.8\%, $+8.3$pp vs Single) and Hard tasks (63.5\%, $+9.0$pp), where Planner-Coder-Critic coordination enables sophisticated decomposition. Notably, Naive outperforms both on Hard tasks (65.0\%), which we attribute to over-refinement in complex architectures introducing bugs.

\begin{table*}[t]
    \centering
    \small
    \caption{Architecture Comparison: Functional Correctness (\%) by Domain and Difficulty. Best per category in \textbf{bold}.}
    \label{tab:architecture-comparison}
    \begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}lccccccccccc}
        \toprule
        & \multicolumn{6}{c}{\textbf{By Domain}} & & \multicolumn{3}{c}{\textbf{By Difficulty}} \\
        \cmidrule(lr){2-7} \cmidrule(lr){9-11}
        \textbf{Architecture} & \textbf{Str} & \textbf{List} & \textbf{Logic} & \textbf{Math} & \textbf{DSA} & \textbf{Avg} && \textbf{Easy} & \textbf{Med} & \textbf{Hard} \\
        \midrule
        Naive & \textbf{93.2} & 71.4 & 67.6 & 56.9 & 83.5 & \textbf{77.6} && 76.8 & 85.5 & \textbf{65.0} \\
        Single-Agent & 85.2 & \textbf{72.9} & 64.9 & 62.7 & 76.5 & 74.6 && \textbf{89.8} & 83.4 & 54.5 \\
        Multi-Agent & 90.9 & 61.4 & \textbf{75.7} & \textbf{64.7} & \textbf{85.9} & \textbf{77.6} && 74.6 & \textbf{91.8} & 63.5 \\
        \midrule
        $\Delta$ (M-S) & $+5.7$ & $-11.4$ & $+10.8$ & $+2.0$ & $+9.4$ & $+3.0$ && $-15.3$ & $+8.3$ & $+9.0$ \\
        \bottomrule
    \end{tabular*}
    \vspace{1mm}
    \parbox{\textwidth}{\footnotesize M=Multi, S=Single. $\Delta$ shows Multi-Agent improvement over Single-Agent (percentage points).}
\end{table*}

\subsection{RQ2: Impact of Coordination Strategies}

Multi-Agent coordination justifies its computational overhead (15--25 LLM calls vs 5--8 for Single-Agent) primarily on medium-to-hard tasks in algorithmically demanding domains. The Planner's structured decomposition and Critic's independent validation provide greatest benefit when tasks require multi-step reasoning (Logic $+10.8$pp, DSA $+9.4$pp). However, coordination overhead hurts performance on simple tasks (Easy $-15.3$pp, Lists $-11.4$pp) where direct generation suffices.

Iterative Coder-Critic refinement converges quickly: 68\% of tasks succeed on first attempt, 24\% on second iteration. The hybrid model configuration (Qwen for generation, DeepSeek for critique) provides effective validation diversity without requiring identical model capabilities.

\subsection{RQ3: Effect of Role Separation}

Modular role separation conditionally improves generation. Benefits concentrate on tasks requiring: (1) careful planning (Logic: constraint satisfaction, DSA: algorithmic design); (2) systematic validation (Medium tasks: 91.8\% pass rate); (3) quality optimization (DSA: lowest complexity, highest maintainability).

However, role separation adds overhead that hurts simple tasks. String manipulation (pattern matching) and Easy tasks (trivial edge cases) do not benefit from multi-stage reasoning. The Planner's detailed decomposition can over-constrain solutions, while the Critic's feedback may reject valid but unconventional approaches.

\textit{[Note: Section 5.3 on code quality metrics (MI, CC) for hard tasks will be added once data is available.]}
